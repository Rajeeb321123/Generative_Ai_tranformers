{"cells":[{"cell_type":"markdown","metadata":{"id":"mBCzk8VphDRg"},"source":["## Packages"]},{"cell_type":"markdown","metadata":{"id":"wLV0BaUDkYvr"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owKD416rp_rb"},"outputs":[],"source":["!python -m pip install --upgrade pip -q\n","!pip install transformers  -q -U\n","!pip install bitsandbytes  -q -U\n","!pip install peft  -q -U\n","!pip install accelerate  -q -U\n","!pip install flash  -q -U\n","!pip install  datasets -q -U\n","!pip install  scipy -q -U\n","!pip install  trl -q -U\n","!pip install  hf_transfer -q -U\n","!pip install  huggingface_hub -q -U\n","!pip install  wandb -q -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3uiIbKKWgdx2"},"outputs":[],"source":["!transformers-cli env"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZ0EMjVOgvHr"},"outputs":[],"source":["## Unsloth install"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Ljmnub2mndD"},"outputs":[],"source":["# # # Empty VRAM\n","# del model\n","\n","# # del trainer\n","# import gc\n","# gc.collect()\n","# gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"AwXcOFXfg-r_"},"source":["## Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLVIFd9lhGNE"},"outputs":[],"source":["# For gated models on HuggingFace\n","# from huggingface_hub import notebook_login\n","# notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrrQWjPmhRsr"},"outputs":[],"source":["%env HF_HUB_ENABLE_HF_TRANSFER = True # for high speed downloading and uploading to hugging face hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"knZB6c-phmPB"},"outputs":[],"source":["cache_dir = '' # comment out if Google Drive is aset as cache_dir\n","\n","# base model (Unsupervised Trial)\n","model_id = \"openchat/openchat_3.5\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7WvkXooiRhB"},"outputs":[],"source":["## Load the model and Tokenizer of LoRA or DoRA\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit = True,\n","    bnb_4bit_use_double_quant = True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16, # if newer gpu: bfloat16\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TDi7OpEfYcq"},"outputs":[],"source":["# config = AutoConfig.from_pretrained(model_id)\n","# cofig.max_position_embeddings = 4096 # (input + output) #model will only learn from max 4096 sequence of token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WyrKD_WhXFn"},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    #config=config,\n","\n","    quantization_config=bnb_config,\n","\n","    #rope_scaling={\"type\":linear, \"factor\": 2.0}, # roPE scaling: https://www.hopsworks.ai/dictionary/rope-scaling and https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/preparing_model\n","\n","    #https://huggingface.co/docs/accelerate/v0.25.0/en/concept_guides/big_model_inference\n","    # device_map='auto', # Itâ€™s fully possible to create your own device map for the layers to use as well, specifying the GPU device to use (a number), \"cpu\", or \"disk\" and pass this in:\n","    device_map = {\"\": 0}, # above auto wasnot working\n","\n","    # Here, the \"trust_remote_code=True\" means \"download the model code from huggingface repo 'internlm/internlm-chat-7b'\", along with the weight, and run it. If it's False, the library would use builtin model architectures hardcoded in huggingface/transformers and only download the weight.\n","    #trust_remote_code=False,\n","\n","    torch_dtype=torch.float16, # if newer gpu: bfloat16\n","\n","    # https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention\n","    # attn_implementation=\"flash_attention_2\", # Works with llama model\n","\n","    cache_dir = cache_dir\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rS-hXeLwoclN"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkm8areWsylC"},"outputs":[],"source":["## Load the Model and Tokenizer for Unsloth"]},{"cell_type":"markdown","metadata":{"id":"Tdt9Gof7s39U"},"source":["## Loading checks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6h6k1m5s7yT"},"outputs":[],"source":["# Check there are no parameter overflowing onto cpu (meta)\n","# Making sure all of the parameter are in GPU not in CPU\n","for n, p in model.named_parameters():\n","  if p.device.type == \"meta\":\n","    print(f\"{n} is on meta\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrO5Z-q8tKbo"},"outputs":[],"source":["print(model.config.max_position_embeddings)\n","\n","#eos = end of sequence\n","# https://huggingface.co/docs/transformers/en/pad_truncation\n","# very important for pad and eos use: https://www.natebrake.com/blog/llm/end-of-sequence-explained\n","print(model.config.eos_token_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkXPSLszUdCy"},"outputs":[],"source":["## Prepare for LoRA fine-tuning\n","def print_trainable_parameters(model):\n","  \"\"\"\n","  Print the number of trainable parameters in the model and lists whic\n","  \"\"\"\n","  trainable_params = 0\n","  non_trainable_params = 0\n","  all_params = 0\n","\n","  print(\"Trainable Parameters:\")\n","  for name, param in model.named_parameters():\n","    # https://www.geeksforgeeks.org/python-pytorch-numel-method/\n","    # Total no of all parameters (trainable + non trainable)\n","    all_params += param.numel() #PyTorch torch.numel() method returns the total number of elements in the input tensor.\n","\n","    # source: copilot: ask about param.requires_drad\n","    # When requires_grad is set to True, it indicates that the parameter participates in gradient computation during backpropagation (i.e., itâ€™s trainable).\n","    #When requires_grad is set to False, the parameter is excluded from gradient updates during training (i.e., itâ€™s frozen).\n","    if param.requires_grad:\n","      trainable_params += param.numel()\n","      print(f\"  {name} \")\n","    else:\n","      non_trainable_params += param.numel()\n","\n","  # This part is same as else portion above but just for printing we did it again\n","  print(\"\\nNon_Trainable Parameters\")\n","  for name, param in model.named_parameters():\n","    if not param.requires_grad:\n","      print(f\" {name} \")\n","\n","\n","  print(\n","      f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params:{non_trainable_params}\"\n","  )"]},{"cell_type":"markdown","metadata":{"id":"Y3P2OKHsYsx1"},"source":["## Standard LoRA or DoRA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OeUh4M73YziD"},"outputs":[],"source":["print(model)"]},{"cell_type":"markdown","metadata":{"id":"4o55adu3aAjj"},"source":["Important documentaion for large model faster training.\n","\n","https://huggingface.co/docs/transformers/v4.18.0/en/performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqZa78QOZaOO"},"outputs":[],"source":["from peft import prepare_model_for_kbit_training\n","\n","# look at gradient checkpointing and gradient accumulation on https://huggingface.co/docs/transformers/v4.18.0/en/performance\n","model.gradient_checkpointing_enable() # to save some memory in VRAM in turn for little slow training\n","\n","model = prepare_model_for_kbit_training(model) # for quantization, must be uncommented.\n","\n","from peft import LoraConfig, get_peft_model\n","\n","# Understaing Lora parameters: https://medium.com/@drishtisharma96505/comparative-analysis-of-lora-parameters-on-llama-2-with-flash-attention-574b913295d4\n","peft_config = LoraConfig( #matching the Llama recipe\n","                         r = 8,\n","                          lora_alpha = 32,\n","                          target_modules = [\n","                              \"q_proj\",\n","                              \"k_proj\",\n","                              \"v_proj\",\n","                              \"o_proj\",\n","                              # \"self_attn.rotary_emb.inv_freq\",\n","\n","                              ## comment out 3 below for mixtril\n","                              \"gate_proj\",\n","                              \"up_proj\",\n","                              \"down_proj\",\n","\n","                              # \"lora_magnitude_vector\" # required for DoRA,\n","                              # \"input_layernorm.weight\",\n","                              # \"post_attention_layernorm.weight\",\n","                              # \"model.norm.weight\",\n","                              # \"lm_head.weight\",\n","\n","\n","                              # \"dense_h_to_4h\",  #for falcon\n","                              # \"dense_4h_to_h\",  #for falcon\n","                              # \"query_key_value\",  #for falcon\n","                              # \"dense\" #for falcon\n","                          ],\n","                          lora_dropout = 0.1,\n","                          bias = \"none\",\n","                          task_type=\"CAUSAL_LM\"\n","                          )\n","\n","model = get_peft_model(model, peft_config) #move to a peft model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vb5daNF7pHHH"},"outputs":[],"source":["# print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUJ8ucWJpTYm"},"outputs":[],"source":["## Unsloth LoRA"]},{"cell_type":"markdown","metadata":{"id":"nNVdqLOOpWaZ"},"source":["## Set up Tokenizer and Padding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVWJ5dAVpds0"},"outputs":[],"source":["print(tokenizer)\n","print(tokenizer.vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-GXSr5XBprg1"},"outputs":[],"source":["print(tokenizer.bos_token) #check begining of sequence\n","print(tokenizer.eos_token) # end of sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IfC82bYqIS7"},"outputs":[],"source":["# # Optionally set the chat template manually.\n","# tokenizer.chat_template = \"{ if not add_generation_prompt is defined %}\"\n","\n","# Test the chat template\n","messages = [\n","    {'role': 'user', 'content': \"write a quick sort algorithm in python\"},\n","    {'role': 'assistant', 'content': \"here your are\"},\n","    {'role': 'user', 'content':\"great.\"}\n","]\n","\n","# When you set tokenize=False in the tokenizer.apply_chat_template() function, it means that the resulting chat template output will not be tokenized into individual tokens. Instead, it remains as a single string without any tokenization. This can be useful when you want to keep the entire chat history intact for further processing or analysis. ðŸ˜Š\n","inputs = tokenizer.apply_chat_template(messages, tokenize = False)\n","print(inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wTTjGDBrp2q"},"outputs":[],"source":["# very important for pad and eos use: https://www.natebrake.com/blog/llm/end-of-sequence-explained\n","# Choosing pad_token for tokenizer\n","\n","## Option A - set the pad token to <pad>, if not <|pad|>, if not <unk> if\n","if '<pad>' in tokenizer.get_vocab():\n","  print('<pad> token is in the tokenizer. Using <pad> for pad')\n","  #set the pad token\n","  tokenizer.pad_token = '<pad>'\n","elif '<|pad|>' in tokenizer.get_vocab():\n","  print('<|pad|> token is in the tokenizer. Using <|pad|> for pad')\n","  #set the pad token\n","  tokenizer.pad_token = '<|pad|>'\n","elif '<unk>' in tokenizer.get_vocab():\n","  print('<unk> token is in the tokenizer. Using unk for pad')\n","  # Set the pad token\n","  tokenizer.pad_token = '<unk>'\n","else: # choosing eos_token as pad_token may be risky.\n","  print(f'Using EOS token, {tokenizer.eos_token}, for padding.')\n","  tokenizer.pad_token = tokenizer.eos_token\n","\n","\n","# ## OPTION B - create pad token\n","# # Check if the pad token is already in the tokenizer vocabulary\n","# if '<pad>' not in tokenizer.get_vocab():\n","#   print('pad token not in the tokenizer, adding a <pad> token')\n","\n","#   #Add the pad token\n","#   tokenizer.add_tokens(['<pad>'])\n","#   # set the pad token\n","#   tokenizer.pad_token = '<pad>'\n","#   # Resize token embeddings\n","#   model.resize_token_embeddings(tokenizer.vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdRrD-Q-vPgq"},"outputs":[],"source":["# Update pad token id in model and it's config\n","model.pad_token_id = tokenizer.pad_token_id\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","# Check if they are equal\n","assert model.pad_token_id == tokenizer.pad_token_id\n","\n","# Print the pad token ids\n","print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n","print('Model pad token ID:', model.pad_token_id)\n","print('Model config pad token ID:', model.config.pad_token_id)\n","print('Number of tokens now in tokenizer:', tokenizer.vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sXi7USFFU9-"},"outputs":[],"source":["print(\"Special tokens map:\", tokenizer.special_tokens_map)\n","print( \"All special tokens:\", tokenizer.all_special_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giXHhFbWWjT-"},"outputs":[],"source":["tokenizer.padding_side = 'right'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1uYpoZjyLDq5"},"outputs":[],"source":["# # Uncomment to switch to left padding, not recommended for unsloth\n","# tokenizer.padding_side = 'left # left padding is ususally not good idea for most model, but some use cases it may be useful"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_vNLYPrLsWB"},"outputs":[],"source":["print(tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"uIJdUBPCMBii"},"source":["## Set embed and norms layers to trainable (recommended only for chat fine tuning if your changing the template or changing the context length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tlTGveezMJG2"},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"2ObmdhqKMWWR"},"source":["## Set up Evaluation\n","\n","- optional"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Bw2hJQKMYyT"},"outputs":[],"source":["from transformers import TextStreamer\n","from peft import PeftModel\n","import torch\n","import gc  # import Python's garbage collection module\n","\n","# Define a stream\n","def Stream(user_prompt, model_type, tokenizer, checkpoint=''):\n","\n","  if model_type == 'base':\n","    eval_model = model\n","  elif model_type == 'fine-tuned':\n","    eval_model = PeftModel.from_pretrained(model, checkpoint)\n","    eval_model = eval_model.to(\"cuda\") # compute in GPU\n","\n","    for n, p in eval_model.named_parameters():\n","      if p.device.type == \"cpu\":\n","        print(f\"{n} is on CPU!\")\n","  else:\n","    print(\"You must set the model_type to base or fine-tuned\")\n","\n","  # print (f'Proceeding to inference with peft adapters from {checkpoint}')\n","\n","  # Source: chatgpt: model.config.use_cache = True\n","  # The use_cache option allows the model to cache intermediate hidden states and attention weights as it generates tokens.\n","  # This cache helps speed up subsequent token generation by reusing previously computed information.\n","  # If youâ€™re generating long sequences or performing autoregressive tasks (where each token depends on previous tokens), enabling cache can significantly improve decoding speed.\n","  eval_model.config.use_cache = True\n","\n","  messages = [\n","      # strip() returns new string with extra(unwanted) white space removed\n","      {'role': 'user', 'content': f\"{user_prompt.strip()}\",}\n","  ]\n","\n","  # add generation prompt must be true for giving ai where to start it's geneartion from in chat prompt eg: this will add at the end: <|im_start|>assistant\n","  inputs = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=True)\n","\n","  # \"pt\" means pytorch tensors When you set return_tensors=\"pt\", the tokenizer or model returns the tokenized input as PyTorch tensors.\n","  #These tensors can be directly used for model inference or fine-tuning.\n","  inputs = tokenizer([inputs], return_tensors=\"pt\", add_special_tokens=False)\n","\n","  # there will be token_type_ids in the end of prompt like Token Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","  # these are useful for many places but not here.\n","  if \"token_type_ids\" in inputs: # we don't token_type_ids here\n","    del inputs[\"token_type_ids\"]\n","\n","  streamer = TextStreamer(tokenizer)\n","\n","  print(f'eval_model is on:',{next(eval_model.parameters()).device}) # CPu or CUDA\n","  print(f'input_ids are on: {inputs[\"input_ids\"].device}')\n","\n","  # parameter of .generate: https://huggingface.co/docs/transformers/en/main_classes/text_generation\n","  _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=10, use_cache=True)\n","\n","  # Clear GPU cache and run garbage collection\n","  torch.cuda.empty_cache() # Clear GPU cache\n","  gc.collect() # Run garbage collection\n","\n","def evaluation(model_type, tokenizer, checkpoint=''):\n","  questions = [\n","    \"In the context of Touch Rugby Internation Rules 2020, what does the dead ball line marks?\",\n","    \"How many players are on the field on each team in touch rugby?\",\n","    \"In touch rugby, does a forward pass result in a roll ball or a Penalty\",\n","    \"In touch rughby, how long is half time?\"\n","    \"In touch rugby, how does the game commence?\"\n","    \"In touch rugby, how many points is a try worth?\"\n","    \"\"\n","  ]\n","\n","  answers = [\n","      \" The Dead ball line marks the end boundaries of the field of play\",\n","      \"6 players\",\n","      \"Penalty\",\n","      \"5 minutes\",\n","      \"The game begins with a tap on the halfway line\"\n","      \"1 point\"\n","  ]\n","\n","  for question, answer in zip(questions, answers):\n","    Stream(question, model_type, tokenizer, checkpoint)\n","    print(\"Correct Answer:\", answer)\n","    print('\\n\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYKKAu-GVPik"},"outputs":[],"source":["print(model.config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkztvnrVYm_X"},"outputs":[],"source":["print(model.generation_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmFGWFZZYquq"},"outputs":[],"source":["# checking the base model which hasnot been fine tunned\n","evaluation(\"base\", tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"18_D1lCTY772"},"source":["## Load the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kc-OZ-BSZILm"},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = 'Trelis/touch-rugby-rules-memorisation'\n","\n","data = load_dataset(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aI_Hfj1kZk4t"},"outputs":[],"source":["# Print frist row of 'train and 'test'\n","print(\"First row of train:\", data['train'][1])\n","print(\"First row of test:\", data['test'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRlLorxbZ9Fk"},"outputs":[],"source":["# Extract text from the first row of 'test' in data\n","text = data['train'][0]['messages']\n","\n","# Tokenize the text\n","tokens = tokenizer.encode(text, add_special_tokens = True)\n","\n","# Decode back to text\n","decoded_text = tokenizer.decode(tokens)\n","\n","# Print the tokens and decode text\n","print(\"Token IDs:\", tokens)\n","print(\"Decode Text:\", decoded_text)"]},{"cell_type":"markdown","metadata":{"id":"oJvSyXmxakNn"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"PyDYGe3ckNLo"},"source":["## Set up and run Training (with saving of data logs to Drive)\n","\n","using TRL trainer is recommended."]},{"cell_type":"markdown","metadata":{"id":"VKV3n4AmkijZ"},"source":["### TRL Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGS0hYR8krw3"},"outputs":[],"source":["model_name = model_id.split(\"/\")[-1]\n","dataset_name = dataset.split(\"/\")[-1]\n","\n","#parameters\n","epochs = 1 # 1 epochs is good enough here\n","context_length = 512 # most of the time Q and A arenot longer than 512\n","\n","# backpropagation params\n","grad_accum = 1 # virtually increase the batch size. Maynot affect VRam but increase Training time\n","batch_size = 1 # just granuale update # smooth and less update for help in memorization\n","\n","fine_tune_tag = 'touch_rugby-rules'\n","save_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{context_length}_length_{grad_accum}_grad_accum_{batch_size}_batch_size_{fine_tune_tag}'\n","print(save_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25dAmFIdlzPw"},"outputs":[],"source":["# #Custom callback for just logging\n","# import transformers\n","# import os\n","\n","# # custom callback to log metrics\n","# class LoggingCallback(transformers.TrainerCallback):\n","#   def _init_(self, log_file_path):\n","#     self.log_file_path = log_file_path\n","#     self.save_dir = save_dir\n","\n","#   def on_log(self, args, state, control, model = None, **kwargs):\n","#     with open(self.log_file_path, 'a') as f:\n","#       if 'loss' in loss:\n","#         f.write(f\"Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\")\n","#       if 'eval_loss' in loss:\n","#         f.write(f\"Step: {state.global_step}, Eval Loss: {logs['eval_loss']}\")\n","\n","#       f.flush() # Force flush the buffered data to file\n","\n","#     # Check if the current step is a checkpoint step\n","#     if state.global_step % int(args.save_steps) == 0:\n","#       # Check if the last checkpoint path exists\n","#       if state.best_model_checkpoint:\n","#         checkpoint_dir = state.best_model_checkpoint\n","#       else:\n","#         # if not, construct the checkpoint directory path manually\n","#         checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint\")\n","\n","#       #Ensure the checkpoint directory exist\n","#       os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","#       # Save trainable params in the checkpoint directory\n","#       current_trainable_params = {n: p for n, p in model.named_parameters()}\n","#       current_trainable_params_state_dict = {n:p.data for n, p in current_trainable_params}\n","\n","#       file_path = os.path.join(checkpoint_dir, \"trainable_params.bin\")\n","#       torch.save(current_trainable_params_state_dict, file_path)\n","\n","# # log file path\n","# log_file_path = os.path.join(cache_dir, \"training_logs.txt\")\n","\n","# # Creating an instance of custom callback class\n","# logging_callback = LoggingCallback(log_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDPUEYLIlgBM"},"outputs":[],"source":["import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTWLWet1nvlK"},"outputs":[],"source":["from transformers import Trainer\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","\n","trainer = SFTTrainer(\n","    # peft_config = peft_config # not needed where as look at above we have already put peft config directly into model but we comment it out\n","    dataset_text_field = \"messages\", # key fied = \"messages\" in dataset in key , value pair\n","    max_seq_length = context_length, # max length of query\n","    tokenizer = tokenizer,\n","    model=model.to(\"cuda\"),\n","    train_dataset = data[\"train\"],\n","    eval_dataset = data[\"test\"],\n","\n","    # about all the parameters: https://huggingface.co/docs/transformers/en/main_classes/trainer\n","    args = TrainingArguments(\n","        max_steps =1, # comment this out after first time you run.\n","        save_steps = 50, ### make sure to check this value is good for our data, The save_steps parameter specifies the number of training steps between consecutive model checkpoints.\n","        num_train_epochs = epochs,\n","        output_dir = save_dir,\n","        evaluation_strategy = \"steps\", # evaluation is done in every eval_steps\n","        do_eval= True,\n","        eval_steps = 0.2,\n","        per_device_eval_batch_size = batch_size,\n","        per_device_train_batch_size = batch_size,\n","        gradient_accumulation_steps = grad_accum,\n","        log_level =\"debug\",\n","        optim = \"adamw_torch\", # if quantization\n","        fp16 = True, # for low end non_ampere Gpu\n","        #bf16 = True, # for only ampere GPU\n","        max_grad_norm = 0.3,#The max_grad_norm value represents the maximum allowed norm (magnitude) of the gradients during backpropagation.. By setting a maximum norm, you prevent gradients from becoming too large, which can lead to unstable training or divergence.\n","\n","        # from chatgpt\n","        # here cosine will be game changer as it decrease validataion loss and against overfitting\n","        lr_scheduler_type = \"cosine\", # follow cosine shaped curve. cosine shape curve make sure lr decrease ove steps.\n","        hub_private_repo = False,\n","\n","        # from chatgpt\n","        # Warmup is an initial phase where the learning rate gradually increases from a very small value to its regular value. (0 to lr)\n","        # It helps stabilize training and allows the model to explore the loss landscape more effectively.\n","        # If you set warmup_ratio = 0.03 and T_max = 1000, the warmup phase will last for the first 30 steps (3% of the total).\n","        warmup_ratio = 0.03,\n","        # optim = \"adamw_torch\", # commented for LoRA+, we are using lora so needed\n","        learning_rate= 1e-4, # comment for LoRA +\n","        report_to=\"tensorboard\",\n","    ),\n","    # ,callbacks = [logging_callback], # if custom callback created\n","    # optimizers = (optimizer, None) # for only LoRA +\n","    # neftune_noise_alpha = 5 # Add in noise embeddings to improve performance\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnHoGdl43Wc6"},"outputs":[],"source":["model.config.use_cache = False # for silencing warnings only\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"Ip1EPFGF3weu"},"source":["# Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W6g2tvW24AZ6"},"outputs":[],"source":["pip install matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulmffrbF4DVe"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Initialize list to hold training and evaluation losses and steps\n","train_losses = []\n","eval_losses = []\n","train_steps = []\n","eval_steps = []\n","\n","#Populate the list from log history\n","#import pandas as pd\n","# pd.DataFrame(trainer.state.log_history)\n","for entry in trainer.state.log_history:\n","  if 'loss' in entry:\n","    train_losses.append(entry['loss'])\n","    train_steps.append(entry['step'])\n","  if 'eval_loss' in entry:\n","    eval_losses.append(entry['eval_loss'])\n","    eval_steps.append(entry['step'])\n","\n","# plot the losses\n","plt.plot(train_steps, train_losses, label = 'Train Loss')\n","plt.plot(eval_steps, eval_losses, label = 'eval Loss')\n","plt.xlabel('Steps')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4ogPiUfFPwkl"},"source":["# Evaluate After Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Swt_0r7VP4ae"},"outputs":[],"source":["# # Can set to true for faster inference\n","# model.config.use_cache = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZQt4gyzP_Xh"},"outputs":[],"source":["evaluation(\"base\", tokenizer) # use this if trained using adapter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHLimdulQLzT"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNcoTRNbvZrcjzJnH/APdUg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}